{
 "metadata": {
  "name": "",
  "signature": "sha256:6295891a330b75ca88dc8b1fb2e88d3c1785eb517db94d5292890285db2f175f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Setting up the Twitter API"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll be working with the [twitter Python package](\"https://pypi.python.org/pypi/twitter\"), which enables us to access the [Twitter API](\"https://dev.twitter.com/docs\") after obtaining proper authorization from [creating a new Twitter application](\"https://apps.twitter.com/\").  Extracting the consumer key, consumer secret, access token key, and access token secret (whatever each of those means) into a text document keeps our code is viewable without giving everyone access to our Twitter account.  `twitter_api.txt` simply has each authentication item on a separate line.  Then, we can do the following:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import twitter\n",
      "mine = [s.strip() for s in open('twitter_api.txt', 'rb').readlines()]\n",
      "twitter_api = twitter.Api(\n",
      "    consumer_key=mine[0],\n",
      "    consumer_secret=mine[1],\n",
      "    access_token_key=mine[2],\n",
      "    access_token_secret=mine[3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use the API to pull out users:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "snames = ['edz504', 'perezhilton']\n",
      "users = twitter_api.UsersLookup(screen_name = snames)\n",
      "[u.statuses_count for u in users]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "[814, 219054]"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "and pull out a maximum of 200 tweets (per request) from a given user's timeline:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "this_user_tweets = twitter_api.GetUserTimeline(screen_name='perezhilton', count=200)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TwitterError",
       "evalue": "[{u'message': u'Rate limit exceeded', u'code': 88}]",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTwitterError\u001b[0m                              Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-3-111eebde123f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mthis_user_tweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtwitter_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetUserTimeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscreen_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'perezhilton'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32mC:\\Program Files\\Python\\lib\\site-packages\\twitter\\api.pyc\u001b[0m in \u001b[0;36mGetUserTimeline\u001b[1;34m(self, user_id, screen_name, since_id, max_id, count, include_rts, trim_user, exclude_replies)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[0mjson\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_RequestUrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'GET'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 623\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ParseAndCheckTwitter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNewFromJsonDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Program Files\\Python\\lib\\site-packages\\twitter\\api.pyc\u001b[0m in \u001b[0;36m_ParseAndCheckTwitter\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m   3411\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3412\u001b[0m       \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimplejson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3413\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_CheckForTwitterError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3414\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3415\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;34m\"<title>Twitter / Over capacity</title>\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Program Files\\Python\\lib\\site-packages\\twitter\\api.pyc\u001b[0m in \u001b[0;36m_CheckForTwitterError\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   3438\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mTwitterError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3439\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'errors'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3440\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mTwitterError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'errors'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3442\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_RequestUrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mTwitterError\u001b[0m: [{u'message': u'Rate limit exceeded', u'code': 88}]"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can pull out up to about 3200 tweets from a given user by repeatedly specifiying a `max_id` parameter.  This method roughly pulls in 200 per request, but Twitter does not fill holes in status ids, so if a user has deleted tweets, a given request may only turn up 195 tweets or similar (print commented out):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getUserTweets(u):\n",
      "    if (u.statuses_count < 1 or u.protected): # don't bother going through if we know the user doesn't have any tweets\n",
      "        # or if they're protected\n",
      "        return([])\n",
      "    else:\n",
      "        user_tweets = []\n",
      "        keep_em_coming = True\n",
      "\n",
      "        while (keep_em_coming):\n",
      "            # for our first time, no max_id\n",
      "            if len(user_tweets) == 0:\n",
      "                this_user_tweets = twitter_api.GetUserTimeline(\n",
      "                    screen_name=u.screen_name,\n",
      "                    count=200)\n",
      "            else:\n",
      "                this_user_tweets = twitter_api.GetUserTimeline(\n",
      "                    screen_name=u.screen_name,\n",
      "                    count=200,\n",
      "                    max_id=oldest_id)\n",
      "            # print 'Pulled in ' + str(len(this_user_tweets)) + ' new tweets'\n",
      "            if len(this_user_tweets) <= 1:\n",
      "                keep_em_coming = False\n",
      "            else:\n",
      "                oldest_id = min([t.id for t in this_user_tweets])\n",
      "            user_tweets += this_user_tweets\n",
      "        return(user_tweets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each status object has a `.text` field that has the data we want.  Since we'll be forming corpora from a set of tweets, and analyzing word frequencies from those corpora, we'll want to clean the tweets.  The method of cleaning we employ here is slightly arbitrary -- since we don't believe screen names count as words, we filter those out:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "def remove_sn(t):\n",
      "    rep = 0\n",
      "    ats_cleaned = True\n",
      "    if ('@' in t):\n",
      "        ats_cleaned = False\n",
      "    while(not ats_cleaned):\n",
      "        at_ind_start = t.index('@')\n",
      "        # any non-screen-name-enabled character\n",
      "        # so if the tweet is \"...@Beyonce:blahhh \"\n",
      "        # then we leave blahhh in the string too\n",
      "        m = re.search(r'\\W', t[at_ind_start + 1:])\n",
      "        if m:\n",
      "            at_ind_end = at_ind_start + m.start()\n",
      "        else:\n",
      "            at_ind_end = at_ind_start + len(t[at_ind_start:])\n",
      "        t = t[:at_ind_start] + t[at_ind_end + 1:]\n",
      "        if (not '@' in t):\n",
      "            ats_cleaned = True\n",
      "        if (rep > 140):\n",
      "            print 'Reached' + str(rep) + ' repetitions for the string:'\n",
      "            print t\n",
      "            print 'at index ' + rep + 'breaking'\n",
      "            break\n",
      "        rep += 1\n",
      "    return(t)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We also filter our URLs and all punctuation (excluding apostrophes and dashes).  Note that this takes out the pound sign in a hashtag, leaving the actual content of the hashtag -- we're making the judgement call to keep that as a word."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def remove_urls(t):\n",
      "    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', t)\n",
      "    for url in urls:\n",
      "        t = re.sub(re.escape(url), '', t)\n",
      "    return(t)\n",
      "    \n",
      "def leave_alphanumeric(t):\n",
      "    return re.sub(r'(?! )(?!-)(?!\\')\\W', '', t)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we want to use the [nltk](\"http://www.nltk.org/\") package to tokenize and assemble a frequency dictionary from a set of tweets."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "\n",
      "# supply either a screen name, or a list of tweets\n",
      "def getUserTweetWordFreqDist(u, user_tweets=None):\n",
      "    if user_tweets is None:\n",
      "        user_tweets = getUserTweets(u)\n",
      "    user_tweets_str = [t.text.encode('utf-8') for t in user_tweets]\n",
      "    user_tweets_clean1 = [tweet_cleaner.remove_sn(t_str1) for t_str1 in user_tweets_str]\n",
      "    user_tweets_clean2 = [tweet_cleaner.remove_urls(t_str2) for t_str2 in user_tweets_clean1]\n",
      "    user_tweets_clean3 = [tweet_cleaner.leave_alphanumeric(t_str3) for t_str3 in user_tweets_clean2]\n",
      "    user_tweets_collapse = ''.join(user_tweets_clean3)\n",
      "    user_tweets_tokens = nltk.word_tokenize(user_tweets_collapse)\n",
      "\n",
      "    fdist = FreqDist(word.lower() for word in user_tweets_tokens)\n",
      "    return(fdist)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    }
   ],
   "metadata": {}
  }
 ]
}