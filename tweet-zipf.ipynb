{
 "metadata": {
  "name": "",
  "signature": "sha256:5ff911c81fe381b566559c0a4bfed066dd38ebce7f2ace17976787e4221eb8c1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Setting up the Twitter API"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll be working with the [twitter Python package](\"https://pypi.python.org/pypi/twitter\"), which enables us to access the [Twitter API](\"https://dev.twitter.com/docs\") after obtaining proper authorization from [creating a new Twitter application](\"https://apps.twitter.com/\").  Extracting the consumer key, consumer secret, access token key, and access token secret (whatever each of those means) into a text document keeps our code is viewable without giving everyone access to our Twitter account.  `twitter_api.txt` simply has each authentication item on a separate line.  Then, we can do the following:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import twitter\n",
      "mine = [s.strip() for s in open('twitter_api.txt', 'rb').readlines()]\n",
      "twitter_api = twitter.Api(\n",
      "    consumer_key=mine[0],\n",
      "    consumer_secret=mine[1],\n",
      "    access_token_key=mine[2],\n",
      "    access_token_secret=mine[3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use the API to pull out users:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "snames = ['edz504', 'perezhilton']\n",
      "users = twitter_api.UsersLookup(screen_name = snames)\n",
      "[u.statuses_count for u in users]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "[814, 219065]"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "and pull out a maximum of 200 tweets (per request) from a given user's timeline:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "this_user_tweets = twitter_api.GetUserTimeline(screen_name='perezhilton', count=200)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can pull out up to about 3200 tweets from a given user by repeatedly specifiying a `max_id` parameter.  This method roughly pulls in 200 per request, but Twitter does not fill holes in status ids, so if a user has deleted tweets, a given request may only turn up 195 tweets or similar (print commented out):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getUserTweets(u):\n",
      "    if (u.statuses_count < 1 or u.protected): # don't bother going through if we know the user doesn't have any tweets\n",
      "        # or if they're protected\n",
      "        return([])\n",
      "    else:\n",
      "        user_tweets = []\n",
      "        keep_em_coming = True\n",
      "\n",
      "        while (keep_em_coming):\n",
      "            # for our first time, no max_id\n",
      "            if len(user_tweets) == 0:\n",
      "                this_user_tweets = twitter_api.GetUserTimeline(\n",
      "                    screen_name=u.screen_name,\n",
      "                    count=200)\n",
      "            else:\n",
      "                this_user_tweets = twitter_api.GetUserTimeline(\n",
      "                    screen_name=u.screen_name,\n",
      "                    count=200,\n",
      "                    max_id=oldest_id)\n",
      "            # print 'Pulled in ' + str(len(this_user_tweets)) + ' new tweets'\n",
      "            if len(this_user_tweets) <= 1:\n",
      "                keep_em_coming = False\n",
      "            else:\n",
      "                oldest_id = min([t.id for t in this_user_tweets])\n",
      "            user_tweets += this_user_tweets\n",
      "        return(user_tweets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each status object has a `.text` field that has the data we want.  Since we'll be forming corpora from a set of tweets, and analyzing word frequencies from those corpora, we'll want to clean the tweets.  The method of cleaning we employ here is slightly arbitrary -- since we don't believe screen names count as words, we filter those out:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "def remove_sn(t):\n",
      "    rep = 0\n",
      "    ats_cleaned = True\n",
      "    if ('@' in t):\n",
      "        ats_cleaned = False\n",
      "    while(not ats_cleaned):\n",
      "        at_ind_start = t.index('@')\n",
      "        # any non-screen-name-enabled character\n",
      "        # so if the tweet is \"...@Beyonce:blahhh \"\n",
      "        # then we leave blahhh in the string too\n",
      "        m = re.search(r'\\W', t[at_ind_start + 1:])\n",
      "        if m:\n",
      "            at_ind_end = at_ind_start + m.start()\n",
      "        else:\n",
      "            at_ind_end = at_ind_start + len(t[at_ind_start:])\n",
      "        t = t[:at_ind_start] + t[at_ind_end + 1:]\n",
      "        if (not '@' in t):\n",
      "            ats_cleaned = True\n",
      "        if (rep > 140):\n",
      "            print 'Reached' + str(rep) + ' repetitions for the string:'\n",
      "            print t\n",
      "            print 'at index ' + rep + 'breaking'\n",
      "            break\n",
      "        rep += 1\n",
      "    return(t)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We also filter our URLs and all punctuation (excluding apostrophes and dashes).  Note that this takes out the pound sign in a hashtag, leaving the actual content of the hashtag -- we're making the judgement call to keep that as a word."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def remove_urls(t):\n",
      "    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', t)\n",
      "    for url in urls:\n",
      "        t = re.sub(re.escape(url), '', t)\n",
      "    return(t)\n",
      "    \n",
      "def leave_alphanumeric(t):\n",
      "    return re.sub(r'(?! )(?!-)(?!\\')\\W', '', t)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we want to use the [nltk](\"http://www.nltk.org/\") package to tokenize and assemble a frequency dictionary from a set of tweets."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "\n",
      "# supply either a screen name, or a list of tweets\n",
      "def getUserTweetWordFreqDist(u, user_tweets=None):\n",
      "    if user_tweets is None:\n",
      "        user_tweets = getUserTweets(u)\n",
      "    user_tweets_str = [t.text.encode('utf-8') for t in user_tweets]\n",
      "    user_tweets_clean1 = [tweet_cleaner.remove_sn(t_str1) for t_str1 in user_tweets_str]\n",
      "    user_tweets_clean2 = [tweet_cleaner.remove_urls(t_str2) for t_str2 in user_tweets_clean1]\n",
      "    user_tweets_clean3 = [tweet_cleaner.leave_alphanumeric(t_str3) for t_str3 in user_tweets_clean2]\n",
      "    user_tweets_collapse = ''.join(user_tweets_clean3)\n",
      "    user_tweets_tokens = nltk.word_tokenize(user_tweets_collapse)\n",
      "\n",
      "    fdist = FreqDist(word.lower() for word in user_tweets_tokens)\n",
      "    return(fdist)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    }
   ],
   "metadata": {}
  }
 ]
}